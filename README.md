# ラビットチャレンジ 機械学習  
  
## 線形回帰モデル  

要点  
・1次式（=直線）で表現されるモデル。  
・慣例として予測値にはハットをつける。  
・パラメータは最小二乗法で決定。  
（但し、外れ値には弱く、他の手法もある。）  
・線形回帰の場合には、最尤法でのパラメータ推定と一致する。  
  
実装演習  
・部屋数4、犯罪率0.3の物件の予測は、4.24。  

 ##非線形回帰モデル  
  
要点  
・ほとんどの実データは線形モデルでは捉えられない。  
・基底関数を用いてモデルを構築。  
・多項式基底、ガウス基底、スプライン基底。  
・モデルの表現力が上がる事でオーバーフィッティングが生じる。  
・対策：①データを増やす ②パラメータの数を調整 ③正則化。  
・クロスバリデーション法やホールドアウト法で汎化性能をチェック。  
  
##ロジスティック回帰モデル  
  
要点  
・分類問題を解く為のモデル。  
・線形結合をシグモイド関数に通す。  
・シグモイド関数の微分はシグモイド関数自身で表せる。  
・確率が0.5以上だと正、o.5未満だと負と定義して最尤推定。  
・ベルヌーイ分布を仮定して最尤推定としてパラメータを推定。  
・パラメータは解析的に求められないので、勾配降下法を求める。  
・評価はRecall、Precision、F値等で行う。  
  
##主成分分析  
  
要点  
・多変量データをより少数のデータで表す。  
・制約付き最小化問題→ラグランジュ未定計数法。  
・元データの分散共分散行列の固有値と固有ベクトルが解となる。  
・分散共分散行列は正定値行列なので固有値は必ず0以上で、固有ベクトルは直行する。  
  
##アルゴリズム  
  
要点  
k近傍法  
・最近傍のデータをk個とってきてその多数決で分類する手法。  
・kを大きくすると決定協会は滑らかになる。  
k-means  
・クラスタ重心からの距離で分類するアルゴリズム。  
・中心の初期値を変えると結果が変わりうる。  
・kの値を変えると結果が変わる。  
  
##サポートベクターマシーン  
  
要点  
・線形判別関数と最も近いデータ点との距離＝マージン。  
・マージンが最大となる線形判別関数を求める。  
・ソフトマージン→誤差に対してペナルティを課す。  
・カーネルトリック→カーネル関数により高次元空間で表現する事で  
非線形な分離が可能になる。  
